{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# class 8 More Regression/Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- [8.1 Other Classifiers](#8.1-Other-Classifiers)\n",
    "    - [8.1.1 K Nearest Neighbors](#8.1.1-K-Nearest-Neighbors)\n",
    "    - [8.1.1.1 Handwritten Digits](#8.1.1.1-Handwritten-Digits)\n",
    "    - [8.1.2 Multinomial Logistic Regression](#8.1.2-Multinomial-Logistic-Regression)\n",
    "    - [8.1.3 ROC Curves](#8.1.3-ROC-Curves)\n",
    "- [8.2 Regularized Regression](#8.2-Regularized-Regression)\n",
    "    - [8.2.0 Variable Selection](#8.2.0-Variable-Selection)\n",
    "    - [8.2.1 L2 Regression](#8.2.1-L2-Regression)\n",
    "    - [8.2.2 L1 Regression](#8.2.2-L1-Regression)\n",
    "    - [8.2.3 ElasticNet](#8.2.3-Elastic-Net)\n",
    "- [8.3 kaggle](#8.3-kaggle)\n",
    "    - [8.3.1 Housing Data](#8.3.1-Housing-Data)\n",
    "    - [8.3.2 Your first submission](#8.3.2-Your-first-submission)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%.3f'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "#from mpl_toolkits.mplot3d import axes3d\n",
    "#import seaborn as sns\n",
    "\n",
    "#import sklearn.linear_model as skl_lm\n",
    "#from sklearn.metrics import mean_squared_error, r2_score\n",
    "#import statsmodels.api as sm\n",
    "#import statsmodels.formula.api as smf\n",
    "\n",
    "#from scipy import stats\n",
    "#from scipy import special\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "\n",
    "%precision 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------\n",
    "### 8.1 Other Classifiers\n",
    "\n",
    "##### 8.1.1 K Nearest Neighbors\n",
    "\n",
    "![KNN](figures/irisDataCampKNN.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=6, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "knn = KNeighborsClassifier(n_neighbors=6)\n",
    "knn.fit(iris['data'], iris['target'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: Split the iris data into training and test. Predict on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Split into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris['data'], iris['target'], test_size = 0.3, random_state=42)\n",
    "\n",
    "# Create a k-NN classifier with 7 neighbors: knn\n",
    "knn = KNeighborsClassifier(n_neighbors=6)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Print the accuracy\n",
    "print(knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Idea, no modeling assumptions at all !!\n",
    "Think about the following:\n",
    "\n",
    "- What is \"the model\", i.e. what needs to be stored ? (coefficients, functions, ...)\n",
    "- What is the model complexity ?\n",
    "- Does this only work for classification ? What would be the regression analogy ?\n",
    "- What improvements could we make to the simple idea ?\n",
    "- In the modeling world:\n",
    "    * linear ?\n",
    "    * local vs. global\n",
    "    * memory/CPU requirements\n",
    "    * wide versus tall data ?\n",
    "\n",
    "\n",
    "###### 8.1.1.1 Handwritten Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f59a539940>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "digits = datasets.load_digits()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#how can I improve the plots ? (i..e no margins, box around plot...)\n",
    "plt.figure(1)\n",
    "\n",
    "for i in np.arange(10)+1:\n",
    "    plt.subplot(2, 5, i)\n",
    "    plt.axis('off')\n",
    "    #plt.gray() \n",
    "    #plt.matshow(digits.images[i-1]) \n",
    "    plt.imshow(digits.images[i-1], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9833333333333333\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create feature and target arrays\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "# Split into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Create a k-NN classifier with 7 neighbors: knn\n",
    "knn = KNeighborsClassifier(n_neighbors=7)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Print the accuracy\n",
    "print(knn.score(X_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Big Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0   0   1   2   3   4   5   6   7   8   9\n",
       "row_0                                        \n",
       "0      36   0   0   0   0   0   0   0   0   0\n",
       "1       0  36   0   0   0   0   0   0   2   0\n",
       "2       0   0  35   0   0   0   0   0   0   0\n",
       "3       0   0   0  37   0   0   0   0   0   0\n",
       "4       0   0   0   0  36   0   0   0   0   1\n",
       "5       0   0   0   0   0  37   0   0   0   0\n",
       "6       0   0   0   0   0   0  35   0   0   0\n",
       "7       0   0   0   0   0   0   0  36   1   0\n",
       "8       0   0   0   0   0   0   1   0  32   1\n",
       "9       0   0   0   0   0   0   0   0   0  34"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = knn.predict(X_test)\n",
    "\n",
    "pd.crosstab(preds,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**\n",
    "\n",
    "Construct a model complexity curve for the digits dataset! In this exercise, you will compute and plot the training and testing accuracy scores for a variety of different neighbor values. By observing how the accuracy scores differ for the training and testing sets with different values of k, you will develop your intuition for overfitting and underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '%.3f'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-1c80302875ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m#Compute accuracy on the training set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mtrain_accuracy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m___\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m#Compute accuracy on the testing set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: '%.3f'"
     ]
    }
   ],
   "source": [
    "# Setup arrays to store train and test accuracies\n",
    "neighbors = np.arange(1, 20)\n",
    "train_accuracy = np.empty(len(neighbors))\n",
    "test_accuracy = np.empty(len(neighbors))\n",
    "\n",
    "# Loop over different values of k\n",
    "for i, k in enumerate(neighbors):\n",
    "    # Setup a k-NN Classifier with k neighbors: knn\n",
    "    knn = KNeighborsClassifier(___)\n",
    "\n",
    "    # Fit the classifier to the training data\n",
    "    ___\n",
    "    \n",
    "    #Compute accuracy on the training set\n",
    "    train_accuracy[i] = ___\n",
    "\n",
    "    #Compute accuracy on the testing set\n",
    "    test_accuracy[i] = ___\n",
    "\n",
    "# Generate plot\n",
    "plt.title('k-NN: Varying Number of Neighbors')\n",
    "plt.plot(___, label = 'Testing Accuracy')\n",
    "plt.plot(___, label = 'Training Accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('Number of Neighbors')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1.2 Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "log_reg = LogisticRegression(multi_class='multinomial',solver='sag', max_iter=100, random_state=42)\n",
    "log_reg.fit(iris[\"data\"][:,3:],iris[\"target\"])\n",
    "    \n",
    "preds = log_reg.predict_proba(iris[\"data\"][:,3:])\n",
    "\n",
    "preds[1:5,:]\n",
    "#log_reg = LogReg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Compute a confusion matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Further Reading and Explorations:**\n",
    "* Read about \"one versus all\" pitted against multinomial. Check out this [notebook](plot_logistic_multinomial.ipynb).\n",
    "* Read about [marginal probabilities](http://data.princeton.edu/wws509/stata/mlogit.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1.3 ROC Curves\n",
    "\n",
    "Simplest to go back to 2 labels from lesson 7:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:,3:]\n",
    "y = (iris[\"target\"]==2).astype(np.int)\n",
    "# Split into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train,y_train)\n",
    "\n",
    "y_pred = log_reg.predict_proba(X_test) \n",
    "y_pred_prob = y_pred[:,1]\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred_prob > 0.25 ))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred_prob > 0.5 ))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred_prob > 0.75 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The need for more sophisticated metrics than accuracy and single thresholding\n",
    "\n",
    "This is particularly relevant for imbalanced classes, example: Emails\n",
    "\n",
    "- Spam classification\n",
    "    * 99% of emails are real; 1% of emails are spam\n",
    "- Could build a classifier that predicts ALL emails as real\n",
    "    * 99% accurate!\n",
    "    * But horrible at actually classifying spam\n",
    "    * Fails at its original purpose\n",
    " \n",
    " ![ConfusionMatrix](figures/DataCampConfusionMatrix.png)\n",
    " \n",
    " ![ISLR-Table4.7](figures/ISLR-Table4.7.png)\n",
    "\n",
    " \n",
    " **Metrics from CM**\n",
    " \n",
    "- Precision: $$\\frac{TP}{TP+FP}$$\n",
    "- Recall: $$\\frac{TP}{TP+FN}$$\n",
    "- F1 score: \n",
    "$$2 \\cdot \\frac{precision \\cdot recall}{precision + recall}$$\n",
    "The F1 score is the harmonic average of the precision and recall, where an F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0. ()\n",
    "- High precision: Not many real emails predicted as spam\n",
    "- High recall: Predicted most spam emails correctly\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_prob > 0.5 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we still need to fix a threshold for any of the metrics above to work !\n",
    "\n",
    "Wouldn't it be best to communicate the prediction quality over a wide range (all!) of thresholds and enable the user to choose the most suitable one for his/her application !\n",
    "That is the idea of the **Receiver Operating Characteristic (ROC)** curve!\n",
    "\n",
    "![ROC](figures/DataCampROCexample.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The pima-indians- diabetes data:\n",
    "#  https://www.kaggle.com/uciml/pima-indians-diabetes-database#diabetes.csv\n",
    "\n",
    "col_names = ['pregnant', 'glucose', 'bp', 'skin', 'insulin', 'bmi', 'pedigree', 'age', 'label']\n",
    "# load dataset\n",
    "pima = pd.read_csv(\"../data/diabetes.csv\", header=None, names=col_names,skiprows=[0])\n",
    "pima.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset in features and target variable\n",
    "feature_cols = ['pregnant', 'insulin', 'bmi', 'age','glucose','bp','pedigree']\n",
    "X = pima[feature_cols] # Features\n",
    "y = pima.label # Target variable\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)\n",
    "\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train,y_train)\n",
    "\n",
    "y_pred = log_reg.predict_proba(X_test) \n",
    "y_pred_prob = y_pred[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr, label='Logistic Regression')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Logistic Regression ROC Curve')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Area under the ROC curve (AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_score(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### AUC using cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv_scores = cross_val_score(log_reg, X, y, cv=10, scoring='roc_auc')\n",
    "\n",
    "print(cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------\n",
    "### 8.2 Regularized Regression\n",
    "\n",
    "We will illustrate the concepts on the Boston housing data set that we are familiar with from lesson 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = pd.read_csv('../data/boston.csv')\n",
    "#print(boston.head())\n",
    "X = boston.drop('medv', axis=1).values\n",
    "y = boston['medv'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2.0 Variable Selection\n",
    "\n",
    "[ISLR slides on model selection](../slides/model_selection.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  8.2.1 L2 Regression\n",
    "\n",
    "Recall: Linear regression minimizes a loss function\n",
    "- It chooses a coefficient for each feature variable\n",
    "- Large coefficients can lead to overfi\"ing\n",
    "- Penalizing large coefficients: Regularization\n",
    "\n",
    "##### Detour: $L_p$ norms\n",
    "\n",
    "http://mathworld.wolfram.com/VectorNorm.html\n",
    "\n",
    "Our new penalty term in finding the coefficients $\\beta_j$ is the minimization\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n{\\left( y_i - \\beta_0 - \\sum_{j=1}^p{\\beta_j x_{ij}} \\right)^2} + \\lambda \\sum_{j=1}^p{\\beta_j^2} = RSS + \\lambda \\sum_{j=1}^p{\\beta_j^2}\n",
    "$$\n",
    "\n",
    "Instead of obtaining **one** set of coefficients we have a dependency of $\\beta_j$  on $\\lambda$:\n",
    "\n",
    "![Ridge Coefficients](figures/RidgeCoefficients1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size = 0.3, random_state=42)\n",
    "ridge = Ridge(alpha=0.1, normalize=True)\n",
    "ridge.fit(X_train, y_train)\n",
    "ridge_pred = ridge.predict(X_test)\n",
    "ridge.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  8.2.2 L1 Regression\n",
    "\n",
    "Our penalty termy looks slightly different (with big consequences for **sparsity**)\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n{\\left( y_i - \\beta_0 - \\sum_{j=1}^p{\\beta_j x_{ij}} \\right)^2} + \\lambda \\sum_{j=1}^p{| \\beta_j |} = RSS + \\lambda \\sum_{j=1}^p{| \\beta_j |}\n",
    "$$\n",
    "\n",
    "![Lasso Coefficients](figures/LassoCoefficients1.png)\n",
    "\n",
    "(Comment: *LASSO* = \"least absolute shrinkage and selection operator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso(alpha=0.1, normalize=True)\n",
    "lasso.fit(X_train, y_train)\n",
    "lasso_pred = lasso.predict(X_test)\n",
    "lasso.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Selection Property of the LASSO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = boston.drop('medv', axis=1).columns\n",
    "lasso_coef = lasso.fit(X, y).coef_\n",
    "_ = plt.plot(range(len(names)), lasso_coef)\n",
    "_ = plt.xticks(range(len(names)), names, rotation=60)\n",
    "_ = plt.ylabel('Coefficients')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tuning the shrinkage parameter with CV\n",
    "\n",
    "(to come, still working on it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2.3 ElasticNet\n",
    "\n",
    "###### The mystery of the additional $\\alpha$ paramater \n",
    "\n",
    "* The elastic net for correlated variables, which uses a penalty that is part L1, part L2.\n",
    "* Compromise between the ridge regression penalty $(\\alpha = 0)$ and the lasso penalty $(\\alpha = 1)$. \n",
    "* This penalty is particularly useful in the $p >> N$ situation, or any situation where there are many correlated predictor variables.\n",
    "\n",
    "\n",
    "$$\n",
    " RSS + \\lambda \\sum_{j=1}^p{ \\left( \\frac{1}{2} (1-\\alpha) \\beta_j^2 + \\alpha | \\beta_j | \\right)}\n",
    "$$\n",
    "The right hand side can be written as\n",
    "$$\n",
    "  \\sum_{j=1}^p{ \\frac{1}{2} \\lambda (1-\\alpha) \\beta_j^2 + \\alpha \\lambda | \\beta_j |} = \\sum_{j=1}^p{  \\lambda_R \\beta_j^2 +  \\lambda_L | \\beta_j |}\n",
    "$$\n",
    "with the Ridge penalty parameter $\\lambda_R \\equiv \\frac{1}{2} \\lambda (1-\\alpha)$ and the lasso penalty parameter $\\lambda_L \\equiv \\alpha \\lambda$.\n",
    "So we see that with\n",
    "$$\n",
    "\\alpha = \\frac{\\lambda_L}{\\lambda_L+ 2 \\lambda_R}, \\mbox{ and } \\lambda= \\lambda_L+ 2 \\lambda_R\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further Reading:\n",
    "- this [notebook](plot_lasso_coordinate_descent_path.ipynb) shows how to plot the entire \"path\" of coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 kaggle\n",
    "####  8.3.1 Housing Data\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This [notebook](../data/kaggle/HousePrices/EDA.ipynb) (despite its annoying \"humor\") is a good start. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  8.3.2 Your first submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
